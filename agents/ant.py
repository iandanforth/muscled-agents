"""
Ant

Learn to control your muscles to make forward progress!

Control Signals

The action space for an ant is continuous control over 16 muscles. For
each of four legs there are four muscles. One leg extensor, one leg flexor,
and two hip muscles which move the leg left and right (or forward and back
depending on your perspective.)

Gym Environment

The `step` method takes an array of 16 values which represent the input to
the fatigable muscle model for each muscle.

PyMuscle Model

The range of valid input values for the muscle model is 0.0 to 1.0.

For example if you send an input value of 0.1 to the muscle model a small number
of weak motor units will be recruited and a small fraction of the total possible
force will be generated by the muscle. If, however, you use an value of 0.95 then
all or nearly all the motor units will be recruited and the muscle will produce
maximal voluntary output.

Output values are normalized by the maximum voluntary force the muscle can
produce. That value is a function of its instantiation characteristics. Output
values will thus be between 0.0 and 1.0.

PyMuscle Fatigue

After use muscles produce less force for the same level of input. So if you
were to send an input signal which recruited all motor units in a muscle
constantly for several seconds the output the model will return will rapidly
decrease. A period of light or no use is required for the muscle to recover.

MuJoCo Model

Each tendon actuator is control limited to the range [-3.0, 0.0]. When a
General actuator is tied to a Tendon in MuJoCo negative values are the
equivalent of contractions. Muscles cannot produce force in extension so no
positive non-zero values are allowed.

The expected input range is [-1.0, 0.0] which is the equivalent of voluntary
force. However real muscles can actually produce 3x voluntary force under
certain circumstances so we reserve that here for future use.

Actuators have a `gainprm` which scales this input value. This is tuned to
a value of 200 to work with the mass of the ant and the resistances of opposing
tendons.


"""
import math
import numpy as np
from collections import deque
from muscledagents.envs.mujoco import MuscledAntEnv

import torch
torch.manual_seed(0)  # set random seed
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


class Policy(nn.Module):
    """
    Code originally from Udacity Deep Reinforcement Learning Examples
    """
    def __init__(self, device, s_size=4, h_size=16, a_size=2):

        super(Policy, self).__init__()
        self.device = device
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.sigmoid(x)

    def act(self, state):
        """
        I think that pytorch is doing a lot of magic here. Eventually
        we want to use the log_prob value returned to start our backward
        pass, which eventually calculates updates to our weights.

        Under the hood it must be linking all the operations on the
        tensors state, probs, m, action and m.log_prob(action)

        By keeping track of how that single value was generated it might be
        inferring how to calculate gradients.
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)


def reinforce(
    env,
    policy,
    optimizer,
    n_episodes=1000,
    max_t=1000,
    gamma=1.0,
    print_every=100
):
    # Collect scores to calculate average performance
    scores_deque = deque(maxlen=100)
    scores = []

    # Train the policy for at most n_episodes
    for i_episode in range(1, n_episodes+1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Collect 1 episode
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break

        reward_total = sum(rewards)
        scores_deque.append(reward_total)  # Last 100 scores
        scores.append(reward_total)  # All scores

        discounts = [gamma**i for i in range(len(rewards)+1)]
        # Calculate sum of discounted rewards
        R = sum([a*b for a, b in zip(discounts, rewards)])  # NOTE: This seems wrong. If you have intermediate rewards why sum them here?

        policy_loss = []
        for log_prob in saved_log_probs:
            policy_loss.append(-log_prob * R)
        policy_loss = torch.cat(policy_loss).sum()

        optimizer.zero_grad()  # Clear old gradients that pytorch may be holding
        policy_loss.backward()  # Calculate gradients
        optimizer.step()  # Update network weights

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))
        if np.mean(scores_deque) >= 195.0:
            print('Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))
            break

    return scores


def main():

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)

    env = MuscledAntEnv()

    print("Observation Space Dims", env.observation_space.shape)
    print("Action Space Dims", env.action_space.shape)

    input_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    policy = Policy(
        device,
        input_size,
        input_size * 6,
        action_size
    )

    # Set up the simulation parameters
    sim_duration = 60  # seconds
    frames_per_second = 50
    step_size = 1 / frames_per_second
    total_steps = int(sim_duration / step_size)

    # Step according to a complex pattern
    # https://www.desmos.com/calculator/c0uq1mul2a
    action = [0.0] * env.muscle_count
    for i in range(total_steps):
        action[1] = ((math.sin(i / 25) + 1) / 2)
        action[5] = ((math.sin(i / 35) + 1) / 2)
        action[9] = ((math.sin(i / 45) + 1) / 2)
        action[13] = ((math.sin(i / 55) + 1) / 2)
        env.step(action)
        env.render()


if __name__ == '__main__':
    main()
